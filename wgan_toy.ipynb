{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "wgan_toy.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunyueLiu/wgan/blob/master/wgan_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5oue0oqCkZZ",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g5RstiiB8V-z",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version 只在 Colab 中使用。\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WZKbyU2-AiY-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "57FFuKn4gLZ9",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YzTlj4YdCip_",
        "colab": {}
      },
      "source": [
        "# 用于生成 GIF 图片\n",
        "!pip install imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YfIk2es3hJEd",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import random\n",
        "import sklearn.datasets\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFvv37IXYyeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODE = 'wgan-gp' # wgan or wgan-gp\n",
        "DATASET = '8gaussians' # 8gaussians, 25gaussians, swissroll\n",
        "DIM = 512 # Model dimensionality\n",
        "FIXED_GENERATOR = False # whether to hold the generator fixed at real data plus\n",
        "                        # Gaussian noise, as in the plots in the paper\n",
        "LAMBDA = .1 # Smaller lambda makes things faster for toy tasks, but isn't\n",
        "            # necessary if you increase CRITIC_ITERS enough\n",
        "CRITIC_ITERS = 5 # How many critic iterations per generator iteration\n",
        "BATCH_SIZE = 256 # Batch size\n",
        "ITERS = 100000 # how many generator iterations to train for"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfHCP6XEbiEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inf_train_gen():\n",
        "    if DATASET == '25gaussians':\n",
        "    \n",
        "        dataset = []\n",
        "        for i in range(100000/25):\n",
        "            for x in range(-2, 3):\n",
        "                for y in range(-2, 3):\n",
        "                    point = np.random.randn(2)*0.05\n",
        "                    point[0] += 2*x\n",
        "                    point[1] += 2*y\n",
        "                    dataset.append(point)\n",
        "        dataset = np.array(dataset, dtype='float32')\n",
        "        np.random.shuffle(dataset)\n",
        "        dataset /= 2.828 # stdev\n",
        "        while True:\n",
        "            for i in range(len(dataset)/BATCH_SIZE):\n",
        "                yield dataset[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
        "\n",
        "    elif DATASET == 'swissroll':\n",
        "\n",
        "        while True:\n",
        "            data = sklearn.datasets.make_swiss_roll(\n",
        "                n_samples=BATCH_SIZE, \n",
        "                noise=0.25\n",
        "            )[0]\n",
        "            data = data.astype('float32')[:, [0, 2]]\n",
        "            data /= 7.5 # stdev plus a little\n",
        "            yield data\n",
        "\n",
        "    elif DATASET == '8gaussians':\n",
        "    \n",
        "        scale = 2.\n",
        "        centers = [\n",
        "            (1,0),\n",
        "            (-1,0),\n",
        "            (0,1),\n",
        "            (0,-1),\n",
        "            (1./np.sqrt(2), 1./np.sqrt(2)),\n",
        "            (1./np.sqrt(2), -1./np.sqrt(2)),\n",
        "            (-1./np.sqrt(2), 1./np.sqrt(2)),\n",
        "            (-1./np.sqrt(2), -1./np.sqrt(2))\n",
        "        ]\n",
        "        centers = [(scale*x,scale*y) for x,y in centers]\n",
        "        while True:\n",
        "            dataset = []\n",
        "            for i in range(BATCH_SIZE):\n",
        "                point = np.random.randn(2)*.02\n",
        "                center = random.choice(centers)\n",
        "                point[0] += center[0]\n",
        "                point[1] += center[1]\n",
        "                dataset.append(point)\n",
        "            dataset = np.array(dataset, dtype='float32')\n",
        "            dataset /= 1.414 # stdev\n",
        "            yield dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9TWcDHLzMtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def Linear( \n",
        "#         input_dim, \n",
        "#         output_dim, \n",
        "#         inputs,\n",
        "#         biases=True,\n",
        "#         initialization=None,\n",
        "#         weightnorm=None,\n",
        "#         gain=1.\n",
        "#         ):\n",
        "#     \"\"\"\n",
        "#     initialization: None, `lecun`, 'glorot', `he`, 'glorot_he', `orthogonal`, `(\"uniform\", range)`\n",
        "#     \"\"\"\n",
        "\n",
        "#       def uniform(stdev, size):\n",
        "#           if _weights_stdev is not None:\n",
        "#               stdev = _weights_stdev\n",
        "#           return np.random.uniform(\n",
        "#               low=-stdev * np.sqrt(3),\n",
        "#               high=stdev * np.sqrt(3),\n",
        "#               size=size\n",
        "#           ).astype('float32')\n",
        "\n",
        "#       if initialization == 'lecun':# and input_dim != output_dim):\n",
        "#           # disabling orth. init for now because it's too slow\n",
        "#           weight_values = uniform(\n",
        "#               np.sqrt(1./input_dim),\n",
        "#               (input_dim, output_dim)\n",
        "#           )\n",
        "\n",
        "#       elif initialization == 'glorot' or (initialization == None):\n",
        "\n",
        "#           weight_values = uniform(\n",
        "#               np.sqrt(2./(input_dim+output_dim)),\n",
        "#               (input_dim, output_dim)\n",
        "#           )\n",
        "\n",
        "#       elif initialization == 'he':\n",
        "\n",
        "#           weight_values = uniform(\n",
        "#               np.sqrt(2./input_dim),\n",
        "#               (input_dim, output_dim)\n",
        "#           )\n",
        "\n",
        "#       elif initialization == 'glorot_he':\n",
        "\n",
        "#           weight_values = uniform(\n",
        "#               np.sqrt(4./(input_dim+output_dim)),\n",
        "#               (input_dim, output_dim)\n",
        "#           )\n",
        "\n",
        "#       elif initialization == 'orthogonal' or \\\n",
        "#           (initialization == None and input_dim == output_dim):\n",
        "          \n",
        "#           # From lasagne\n",
        "#           def sample(shape):\n",
        "#               if len(shape) < 2:\n",
        "#                   raise RuntimeError(\"Only shapes of length 2 or more are \"\n",
        "#                                       \"supported.\")\n",
        "#               flat_shape = (shape[0], np.prod(shape[1:]))\n",
        "#                 # TODO: why normal and not uniform?\n",
        "#               a = np.random.normal(0.0, 1.0, flat_shape)\n",
        "#               u, _, v = np.linalg.svd(a, full_matrices=False)\n",
        "#               # pick the one with the correct shape\n",
        "#               q = u if u.shape == flat_shape else v\n",
        "#               q = q.reshape(shape)\n",
        "#               return q.astype('float32')\n",
        "#           weight_values = sample((input_dim, output_dim))\n",
        "      \n",
        "#       elif initialization[0] == 'uniform':\n",
        "      \n",
        "#           weight_values = np.random.uniform(\n",
        "#               low=-initialization[1],\n",
        "#               high=initialization[1],\n",
        "#               size=(input_dim, output_dim)\n",
        "#           ).astype('float32')\n",
        "\n",
        "#       else:\n",
        "\n",
        "#           raise Exception('Invalid initialization!')\n",
        "\n",
        "#       weight_values *= gain\n",
        "\n",
        "#       weight = weight_values\n",
        "\n",
        "#       if weightnorm==None:\n",
        "#         weightnorm = _default_weightnorm\n",
        "#       if weightnorm:\n",
        "#         norm_values = np.sqrt(np.sum(np.square(weight_values), axis=0))\n",
        "#           # norm_values = np.linalg.norm(weight_values, axis=0)\n",
        "\n",
        "#         target_norms = norm_values\n",
        "#         norms = tf.sqrt(tf.reduce_sum(tf.square(weight), reduction_indices=[0]))\n",
        "#         weight = weight * (target_norms / norms)\n",
        "\n",
        "#         # if 'Discriminator' in name:\n",
        "#         #     print \"WARNING weight constraint on {}\".format(name)\n",
        "#         #     weight = tf.nn.softsign(10.*weight)*.1\n",
        "\n",
        "#       # if inputs.get_shape().ndims == 2:\n",
        "#       #     result = tf.matmul(inputs, weight)\n",
        "#       # else:\n",
        "#       #     reshaped_inputs = tf.reshape(inputs, [-1, input_dim])\n",
        "#       #     result = tf.matmul(reshaped_inputs, weight)\n",
        "#       #     result = tf.reshape(result, tf.pack(tf.unpack(tf.shape(inputs))[:-1] + [output_dim]))\n",
        "\n",
        "#       # if biases:\n",
        "#       #     result = tf.nn.bias_add(\n",
        "#       #         result,\n",
        "#       #         lib.param(\n",
        "#       #             name + '.b',\n",
        "#       #             np.zeros((output_dim,), dtype='float32')\n",
        "#       #         )\n",
        "#       #     )\n",
        "#     result = layers.Dense(units=output_dim, use_bias=biases, )\n",
        "#     return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6bpTcDqoLWjY",
        "colab": {}
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(512, use_bias=True,input_shape=(2,), kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Dense(512, kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Dense(512, kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Dense(2))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "使用（尚未训练的）生成器创建一张图片。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o6VvUbMqgLaS",
        "colab": {}
      },
      "source": [
        "generator_test = make_generator_model()\n",
        "noise = tf.random.normal([5,2])\n",
        "generated_distribution = generator_test(noise, training=False)\n",
        "# plt.imshow(generated_image, cmap='gray')\n",
        "n_distribution = generated_distribution.numpy()\n",
        "plt.scatter(n_distribution[:,0], n_distribution[:,1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### 判别器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dw2tPLmk2pEP",
        "colab": {}
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(512, use_bias=True, input_shape=(2,), kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Dense(512, kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Dense(512, kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Dense(1, kernel_initializer=tf.keras.initializers.he_uniform()))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QhPneagzCaQv"
      },
      "source": [
        "使用（尚未训练的）判别器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nnSVbzhgLaX",
        "colab": {}
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = tf.reshape(discriminator(generated_distribution),[-1])\n",
        "decision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## 定义损失函数和优化器\n",
        "\n",
        "为两个模型定义损失函数和优化器。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wkMNfBWlT-PV",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    disc_cost = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "    return disc_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "### 生成器损失\n",
        "\n",
        "生成器损失量化其欺骗判别器的能力。直观来讲，如果生成器表现良好，判别器将会把伪造图片判断为真实图片（或 1）。这里我们将把判别器在生成图片上的判断结果与一个值全为 1 的数组进行对比。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90BIcCKcDMxz",
        "colab": {}
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return -tf.reduce_mean(fake_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MgIc7i0th_Iu"
      },
      "source": [
        "由于我们需要分别训练两个网络，判别器和生成器的优化器是不同的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, \n",
        "        beta_1=0.5, \n",
        "        beta_2=0.9)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, \n",
        "        beta_1=0.5, \n",
        "        beta_2=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## 定义训练循环\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NS2GWywBbAWo",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100000\n",
        "noise_dim = 2\n",
        "num_examples_to_generate = 256\n",
        "\n",
        "\n",
        "# 我们将重复使用该种子（因此在动画 GIF 中更容易可视化进度）\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E7OZucEoHYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frame_index = [0]\n",
        "def generate_image(true_dist, generator,discriminator):\n",
        "    \"\"\"\n",
        "    Generates and saves a plot of the true distribution, the generator, and the\n",
        "    critic.\n",
        "    \"\"\"\n",
        "    N_POINTS = 128\n",
        "    RANGE = 3\n",
        "\n",
        "    points = np.zeros((N_POINTS, N_POINTS, 2), dtype='float32')\n",
        "    points[:,:,0] = np.linspace(-RANGE, RANGE, N_POINTS)[:,None]\n",
        "    points[:,:,1] = np.linspace(-RANGE, RANGE, N_POINTS)[None,:]\n",
        "    points = points.reshape((-1,2))\n",
        "    fake_data = generator(points, training=False).numpy()\n",
        "    disc_real = discriminator(true_dist,training=False).numpy()\n",
        "    disc_map = discriminator(points, training=False).numpy()\n",
        "    # samples, disc_map = session.run(\n",
        "    #     [fake_data, disc_real], \n",
        "    #     feed_dict={real_data:points}\n",
        "    # )\n",
        "    # disc_map = session.run(disc_real, feed_dict={real_data:points})\n",
        "\n",
        "    plt.clf()\n",
        "\n",
        "    x = y = np.linspace(-RANGE, RANGE, N_POINTS)\n",
        "    plt.contour(x,y,disc_map.reshape((len(x), len(y))).transpose())\n",
        "\n",
        "    plt.scatter(true_dist[:, 0], true_dist[:, 1], c='orange',  marker='+')\n",
        "    plt.scatter(fake_data[:, 0], fake_data[:, 1], c='green', marker='+')\n",
        "    \n",
        "    plt.savefig('frame'+str(frame_index[0])+'.png')\n",
        "    plt.show()\n",
        "    frame_index[0] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3t5ibNo05jCB",
        "colab": {}
      },
      "source": [
        "# 注意 `tf.function` 的使用\n",
        "# 该注解使函数被“编译”\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_data):\n",
        "    # initial variables for weight penalty\n",
        "    alpha = tf.random.uniform(\n",
        "          shape=[BATCH_SIZE,1], \n",
        "          minval=0.,\n",
        "          maxval=1.,\n",
        "        ) \n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    # interpolates = (alpha * real_data + ((1-alpha) * noise))\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_data = generator(noise, training=True)\n",
        "      fake_output = tf.reshape(discriminator(generated_data, training=True),[-1])\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      interpolates = (alpha * real_data + ((1-alpha) * generated_data))\n",
        "      with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolates)\n",
        "        real_output_0 = discriminator(real_data, training=True)\n",
        "        real_output = tf.reshape(real_output_0,[-1])\n",
        "        disc_interpolates = discriminator(interpolates, training=False)\n",
        "      gradients = tape.gradient(disc_interpolates, interpolates)\n",
        "      # print(gradients)\n",
        "      slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients)))\n",
        "      gradient_penalty = tf.reduce_mean((slopes-1)**2)\n",
        "\n",
        "      # discriminate loss penalty\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)+ LAMBDA*gradient_penalty\n",
        "    # get the graients\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWeKlbiYJO4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2M7LmLtGEMQJ",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    gen = inf_train_gen()\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(CRITIC_ITERS):\n",
        "        _data = next(gen)\n",
        "        _data = tf.convert_to_tensor(_data, dtype=tf.float32)\n",
        "        train_step(_data)\n",
        "      # 每 15 个 epoch 保存一次模型\n",
        "      if (epoch + 1) % 15 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      if epoch % 100 == 99:\n",
        "        generate_image(_data, generator, discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2aFF7Hk3XdeW"
      },
      "source": [
        "**生成与保存图片**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ly3UN0SLLY2l",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train(EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rfM4YcPVPkNO"
      },
      "source": [
        "恢复最新的检查点。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SKeqh9GlgLa7",
        "colab": {}
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4M_vIbUi7c0"
      },
      "source": [
        "## 创建 GIF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WfO5wCdclHGL",
        "colab": {}
      },
      "source": [
        "# 使用 epoch 数生成单张图片\n",
        "# def display_image(epoch_no):\n",
        "#   return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TsUudzLCgLbA",
        "colab": {}
      },
      "source": [
        "# display_image(EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NywiH3nL8guF"
      },
      "source": [
        "使用训练过程中生成的图片通过 `imageio` 生成动态 gif "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IGKQgENQ8lEI",
        "colab": {}
      },
      "source": [
        "anim_file = 'wgan_swiss_2.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('frame*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  last = -1\n",
        "  for i,filename in enumerate(filenames):\n",
        "    if i % 10 !=0:\n",
        "      continue\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "\n",
        "import IPython\n",
        "if IPython.version_info > (6,2,0,''):\n",
        "  display.Image(filename=anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGhC3-fMWSwl"
      },
      "source": [
        "如果您正在使用 Colab，您可以通过如下代码下载动画："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uV0yiKpzNP1b",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download(anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k6qC-SbjK0yW"
      },
      "source": [
        "## 下一步\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xjjkT9KAK6H7"
      },
      "source": [
        "本教程展示了实现和训练 GAN 模型所需的全部必要代码。接下来，您可能想尝试其他数据集，例如大规模名人面部属性（CelebA）数据集 [在 Kaggle 上获取](https://www.kaggle.com/jessicali9530/celeba-dataset/home)。要了解更多关于 GANs 的信息，我们推荐参阅 [NIPS 2016 教程： 生成对抗网络](https://arxiv.org/abs/1701.00160)。"
      ]
    }
  ]
}